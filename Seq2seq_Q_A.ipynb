{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20360"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "lines= pd.read_csv('datasets/WikiQACorpus/WikiQACorpus/WikiQA-train.txt', names=['question', 'answer' ,'no'], sep='\\t')\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how are glacier caves formed ?</td>\n",
       "      <td>A partly submerged glacier cave on Perito More...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how are glacier caves formed ?</td>\n",
       "      <td>The ice facade is approximately 60 m high</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how are glacier caves formed ?</td>\n",
       "      <td>Ice formations in the Titlis glacier cave</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how are glacier caves formed ?</td>\n",
       "      <td>A glacier cave is a cave formed within the ice...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how are glacier caves formed ?</td>\n",
       "      <td>Glacier caves are often called ice caves , but...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20355</th>\n",
       "      <td>what is section eight housing</td>\n",
       "      <td>A tenant who leaves a subsidized project will ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20356</th>\n",
       "      <td>what is section eight housing</td>\n",
       "      <td>The United States Department of Housing and Ur...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20357</th>\n",
       "      <td>what is the main type of restaurant</td>\n",
       "      <td>Restaurants categorized by type and informatio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20358</th>\n",
       "      <td>what is us dollar worth based on</td>\n",
       "      <td>U.S. Federal Reserve notes in the mid-1990s</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20359</th>\n",
       "      <td>what is us dollar worth based on</td>\n",
       "      <td>The history of the United States dollar covers...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20360 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  question  \\\n",
       "0           how are glacier caves formed ?   \n",
       "1           how are glacier caves formed ?   \n",
       "2           how are glacier caves formed ?   \n",
       "3           how are glacier caves formed ?   \n",
       "4           how are glacier caves formed ?   \n",
       "...                                    ...   \n",
       "20355        what is section eight housing   \n",
       "20356        what is section eight housing   \n",
       "20357  what is the main type of restaurant   \n",
       "20358     what is us dollar worth based on   \n",
       "20359     what is us dollar worth based on   \n",
       "\n",
       "                                                  answer  no  \n",
       "0      A partly submerged glacier cave on Perito More...   0  \n",
       "1              The ice facade is approximately 60 m high   0  \n",
       "2              Ice formations in the Titlis glacier cave   0  \n",
       "3      A glacier cave is a cave formed within the ice...   1  \n",
       "4      Glacier caves are often called ice caves , but...   0  \n",
       "...                                                  ...  ..  \n",
       "20355  A tenant who leaves a subsidized project will ...   0  \n",
       "20356  The United States Department of Housing and Ur...   0  \n",
       "20357  Restaurants categorized by type and informatio...   0  \n",
       "20358        U.S. Federal Reserve notes in the mid-1990s   0  \n",
       "20359  The history of the United States dollar covers...   0  \n",
       "\n",
       "[20360 rows x 3 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4728</th>\n",
       "      <td>how old julio cesar chavez when he fought de l...</td>\n",
       "      <td>Chávez also has the longest undefeated streak ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16459</th>\n",
       "      <td>what is the resolution of a standard dvd ?</td>\n",
       "      <td>Typically , the data rate for DVD movies range...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6623</th>\n",
       "      <td>how many innings makes an official game</td>\n",
       "      <td>This is approximately the halfway point of the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4243</th>\n",
       "      <td>what countries legalize marijuana</td>\n",
       "      <td>The legality of cannabis varies from country t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16874</th>\n",
       "      <td>what is the word for lung cancer ?</td>\n",
       "      <td>Not all tumors are cancerous .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7370</th>\n",
       "      <td>what are non-agency bond</td>\n",
       "      <td>Agency securities are usually exempt from stat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18512</th>\n",
       "      <td>what is general average sacrifice</td>\n",
       "      <td>`` 2nd . There must be a voluntary jettison , ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7323</th>\n",
       "      <td>what are use taxes ?</td>\n",
       "      <td>With few exceptions , no state 's vendors will...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11324</th>\n",
       "      <td>what are the colors of newfoundland and labrador</td>\n",
       "      <td>On December 6 , 2001 , an amendment was made t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14410</th>\n",
       "      <td>what is go daddy.com ?</td>\n",
       "      <td>On June 24 , 2011 , The Wall Street Journal re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question  \\\n",
       "4728   how old julio cesar chavez when he fought de l...   \n",
       "16459         what is the resolution of a standard dvd ?   \n",
       "6623             how many innings makes an official game   \n",
       "4243                   what countries legalize marijuana   \n",
       "16874                 what is the word for lung cancer ?   \n",
       "7370                            what are non-agency bond   \n",
       "18512                  what is general average sacrifice   \n",
       "7323                                what are use taxes ?   \n",
       "11324   what are the colors of newfoundland and labrador   \n",
       "14410                             what is go daddy.com ?   \n",
       "\n",
       "                                                  answer  no  \n",
       "4728   Chávez also has the longest undefeated streak ...   0  \n",
       "16459  Typically , the data rate for DVD movies range...   0  \n",
       "6623   This is approximately the halfway point of the...   0  \n",
       "4243   The legality of cannabis varies from country t...   0  \n",
       "16874                     Not all tumors are cancerous .   0  \n",
       "7370   Agency securities are usually exempt from stat...   0  \n",
       "18512  `` 2nd . There must be a voluntary jettison , ...   0  \n",
       "7323   With few exceptions , no state 's vendors will...   0  \n",
       "11324  On December 6 , 2001 , an amendment was made t...   0  \n",
       "14410  On June 24 , 2011 , The Wall Street Journal re...   0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[0:20000] # 2만개만 저장\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14098</th>\n",
       "      <td>who is on the hundred dollar bill</td>\n",
       "      <td>\\t On April 24 , 2013 , the Federal Reserve an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1813</th>\n",
       "      <td>what does the universal law of gravitation state</td>\n",
       "      <td>\\t Newton 's law of universal gravitation stat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9060</th>\n",
       "      <td>what year did peyton manning get drafted</td>\n",
       "      <td>\\t He was chosen by the Indianapolis Colts wit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14849</th>\n",
       "      <td>who was charged with murder after the massacre...</td>\n",
       "      <td>\\t While 26 U.S. soldiers were initially charg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14940</th>\n",
       "      <td>what is modified agi</td>\n",
       "      <td>\\t Adjusted gross income is gross income less ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10391</th>\n",
       "      <td>Who do they say the Son of Man ...</td>\n",
       "      <td>\\t It has diverse meanings , ranging from a no...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>how long is a flat membrane roof good for ?</td>\n",
       "      <td>\\t Any sheet of material used to cover a flat ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16316</th>\n",
       "      <td>What Is Argentina Known For</td>\n",
       "      <td>\\t Within Latin America , Argentina has the fi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15628</th>\n",
       "      <td>when was steven tyler born</td>\n",
       "      <td>\\t After Aerosmith launched a remarkable comeb...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18680</th>\n",
       "      <td>what was the first police department in the Un...</td>\n",
       "      <td>\\t Apart from maintaining order and service fu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question  \\\n",
       "14098                  who is on the hundred dollar bill   \n",
       "1813    what does the universal law of gravitation state   \n",
       "9060            what year did peyton manning get drafted   \n",
       "14849  who was charged with murder after the massacre...   \n",
       "14940                               what is modified agi   \n",
       "10391                 Who do they say the Son of Man ...   \n",
       "299          how long is a flat membrane roof good for ?   \n",
       "16316                        What Is Argentina Known For   \n",
       "15628                         when was steven tyler born   \n",
       "18680  what was the first police department in the Un...   \n",
       "\n",
       "                                                  answer  no  \n",
       "14098  \\t On April 24 , 2013 , the Federal Reserve an...   0  \n",
       "1813   \\t Newton 's law of universal gravitation stat...   1  \n",
       "9060   \\t He was chosen by the Indianapolis Colts wit...   0  \n",
       "14849  \\t While 26 U.S. soldiers were initially charg...   1  \n",
       "14940  \\t Adjusted gross income is gross income less ...   0  \n",
       "10391  \\t It has diverse meanings , ranging from a no...   0  \n",
       "299    \\t Any sheet of material used to cover a flat ...   0  \n",
       "16316  \\t Within Latin America , Argentina has the fi...   0  \n",
       "15628  \\t After Aerosmith launched a remarkable comeb...   0  \n",
       "18680  \\t Apart from maintaining order and service fu...   0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.answer = lines.answer.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 글자 집합 구축\n",
    "question_vocab=set()\n",
    "for line in lines.question: # 1줄씩 읽음\n",
    "    for char in line: # 1개의 글자씩 읽음\n",
    "        question_vocab.add(char)\n",
    "\n",
    "answer_vocab=set()\n",
    "for line in lines.answer:\n",
    "    for char in line:\n",
    "        answer_vocab.add(char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "295\n"
     ]
    }
   ],
   "source": [
    "question_vocab_size = len(question_vocab)+1\n",
    "answer_vocab_size = len(answer_vocab)+1\n",
    "print(question_vocab_size)\n",
    "print(answer_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p']\n",
      "['L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n"
     ]
    }
   ],
   "source": [
    "question_vocab = sorted(list(question_vocab))\n",
    "answer_vocab = sorted(list(answer_vocab))\n",
    "print(question_vocab[45:75])\n",
    "print(answer_vocab[45:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '#': 3, '$': 4, '&': 5, \"'\": 6, '(': 7, ')': 8, '*': 9, '+': 10, ',': 11, '-': 12, '.': 13, '/': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, ';': 26, '>': 27, '?': 28, '@': 29, 'A': 30, 'B': 31, 'C': 32, 'D': 33, 'E': 34, 'F': 35, 'G': 36, 'H': 37, 'I': 38, 'J': 39, 'K': 40, 'L': 41, 'M': 42, 'N': 43, 'O': 44, 'P': 45, 'Q': 46, 'R': 47, 'S': 48, 'T': 49, 'U': 50, 'V': 51, 'W': 52, 'X': 53, 'Y': 54, 'Z': 55, '[': 56, '\\\\': 57, ']': 58, '`': 59, 'a': 60, 'b': 61, 'c': 62, 'd': 63, 'e': 64, 'f': 65, 'g': 66, 'h': 67, 'i': 68, 'j': 69, 'k': 70, 'l': 71, 'm': 72, 'n': 73, 'o': 74, 'p': 75, 'q': 76, 'r': 77, 's': 78, 't': 79, 'u': 80, 'v': 81, 'w': 82, 'x': 83, 'y': 84, 'z': 85}\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '#': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, '*': 12, '+': 13, ',': 14, '-': 15, '.': 16, '/': 17, '0': 18, '1': 19, '2': 20, '3': 21, '4': 22, '5': 23, '6': 24, '7': 25, '8': 26, '9': 27, ':': 28, ';': 29, '<': 30, '=': 31, '>': 32, '?': 33, '@': 34, 'A': 35, 'B': 36, 'C': 37, 'D': 38, 'E': 39, 'F': 40, 'G': 41, 'H': 42, 'I': 43, 'J': 44, 'K': 45, 'L': 46, 'M': 47, 'N': 48, 'O': 49, 'P': 50, 'Q': 51, 'R': 52, 'S': 53, 'T': 54, 'U': 55, 'V': 56, 'W': 57, 'X': 58, 'Y': 59, 'Z': 60, '[': 61, '\\\\': 62, ']': 63, '_': 64, '`': 65, 'a': 66, 'b': 67, 'c': 68, 'd': 69, 'e': 70, 'f': 71, 'g': 72, 'h': 73, 'i': 74, 'j': 75, 'k': 76, 'l': 77, 'm': 78, 'n': 79, 'o': 80, 'p': 81, 'q': 82, 'r': 83, 's': 84, 't': 85, 'u': 86, 'v': 87, 'w': 88, 'x': 89, 'y': 90, 'z': 91, '{': 92, '|': 93, '}': 94, '~': 95, '¢': 96, '£': 97, '§': 98, '\\xad': 99, '®': 100, '°': 101, '±': 102, '·': 103, 'Á': 104, 'É': 105, '×': 106, 'Ø': 107, 'à': 108, 'á': 109, 'â': 110, 'ã': 111, 'ä': 112, 'æ': 113, 'ç': 114, 'è': 115, 'é': 116, 'ê': 117, 'ë': 118, 'í': 119, 'ï': 120, 'ð': 121, 'ñ': 122, 'ó': 123, 'ô': 124, 'ö': 125, 'ø': 126, 'ù': 127, 'ú': 128, 'û': 129, 'ü': 130, 'Ā': 131, 'ā': 132, 'ą': 133, 'ć': 134, 'č': 135, 'Đ': 136, 'ē': 137, 'ī': 138, 'ł': 139, 'ň': 140, 'ō': 141, 'Ś': 142, 'ş': 143, 'š': 144, 'ū': 145, 'Ǝ': 146, 'ư': 147, 'ǎ': 148, 'ɔ': 149, 'ɪ': 150, 'ʌ': 151, 'ʔ': 152, 'ʻ': 153, 'ʾ': 154, 'ʿ': 155, 'ˀ': 156, 'ˈ': 157, 'ά': 158, 'έ': 159, 'ή': 160, 'ί': 161, 'α': 162, 'β': 163, 'γ': 164, 'ε': 165, 'ζ': 166, 'η': 167, 'θ': 168, 'ι': 169, 'κ': 170, 'λ': 171, 'μ': 172, 'ν': 173, 'ξ': 174, 'ο': 175, 'π': 176, 'ρ': 177, 'ς': 178, 'σ': 179, 'τ': 180, 'υ': 181, 'φ': 182, 'χ': 183, 'ψ': 184, 'ω': 185, 'ό': 186, 'ύ': 187, 'ώ': 188, 'А': 189, 'а': 190, 'к': 191, 'л': 192, 'с': 193, 'я': 194, 'א': 195, 'ב': 196, 'ד': 197, 'ה': 198, 'ו': 199, 'ז': 200, 'ח': 201, 'ט': 202, 'י': 203, 'ל': 204, 'ם': 205, 'מ': 206, 'ן': 207, 'ק': 208, 'ר': 209, 'ש': 210, 'र': 211, 'श': 212, 'ी': 213, '्': 214, 'ช': 215, 'ม': 216, 'ย': 217, 'ร': 218, 'ว': 219, 'ศ': 220, 'า': 221, 'ิ': 222, 'ี': 223, 'เ': 224, 'ም': 225, 'ሴ': 226, 'ḥ': 227, 'ṭ': 228, 'ạ': 229, 'ả': 230, 'ấ': 231, 'ễ': 232, 'ệ': 233, 'ố': 234, 'ồ': 235, 'ộ': 236, 'ờ': 237, 'ủ': 238, 'ỹ': 239, 'ἀ': 240, 'Ἀ': 241, 'ἐ': 242, 'ἱ': 243, 'Ἰ': 244, 'ὁ': 245, 'ὐ': 246, 'ὰ': 247, 'ὶ': 248, 'ὸ': 249, 'ὺ': 250, '‐': 251, '–': 252, '—': 253, '‘': 254, '’': 255, '“': 256, '”': 257, '†': 258, '•': 259, '‰': 260, '′': 261, '⁄': 262, '€': 263, '−': 264, '≈': 265, '≤': 266, '≥': 267, '┌': 268, '┐': 269, '★': 270, '♀': 271, '♂': 272, '♋': 273, '♙': 274, '♟': 275, 'が': 276, 'な': 277, 'ひ': 278, 'ら': 279, 'カ': 280, 'タ': 281, 'ナ': 282, '一': 283, '劉': 284, '勝': 285, '圖': 286, '安': 287, '小': 288, '李': 289, '萬': 290, '言': 291, '龍': 292, '원': 293, '학': 294}\n"
     ]
    }
   ],
   "source": [
    "#글자에 index를 적용\n",
    "question_to_index = dict([(word, i+1) for i, word in enumerate(question_vocab)])\n",
    "answer_to_index = dict([(word, i+1) for i, word in enumerate(answer_vocab)])\n",
    "print(question_to_index)\n",
    "print(answer_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[67, 74, 82, 1, 60, 77, 64, 1, 66, 71, 60, 62, 68, 64, 77, 1, 62, 60, 81, 64, 78, 1, 65, 74, 77, 72, 64, 63, 1, 28], [67, 74, 82, 1, 60, 77, 64, 1, 66, 71, 60, 62, 68, 64, 77, 1, 62, 60, 81, 64, 78, 1, 65, 74, 77, 72, 64, 63, 1, 28], [67, 74, 82, 1, 60, 77, 64, 1, 66, 71, 60, 62, 68, 64, 77, 1, 62, 60, 81, 64, 78, 1, 65, 74, 77, 72, 64, 63, 1, 28], [67, 74, 82, 1, 60, 77, 64, 1, 66, 71, 60, 62, 68, 64, 77, 1, 62, 60, 81, 64, 78, 1, 65, 74, 77, 72, 64, 63, 1, 28], [67, 74, 82, 1, 60, 77, 64, 1, 66, 71, 60, 62, 68, 64, 77, 1, 62, 60, 81, 64, 78, 1, 65, 74, 77, 72, 64, 63, 1, 28]]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = []\n",
    "for line in lines.question: #입력 데이터에서 1줄씩 문장을 읽음\n",
    "    temp_X = []\n",
    "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
    "      temp_X.append(question_to_index[w]) # 글자를 해당되는 정수로 변환\n",
    "    encoder_input.append(temp_X)\n",
    "print(encoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 35, 3, 81, 66, 83, 85, 77, 90, 3, 84, 86, 67, 78, 70, 83, 72, 70, 69, 3, 72, 77, 66, 68, 74, 70, 83, 3, 68, 66, 87, 70, 3, 80, 79, 3, 50, 70, 83, 74, 85, 80, 3, 47, 80, 83, 70, 79, 80, 3, 41, 77, 66, 68, 74, 70, 83, 3, 16, 3, 2], [1, 3, 54, 73, 70, 3, 74, 68, 70, 3, 71, 66, 68, 66, 69, 70, 3, 74, 84, 3, 66, 81, 81, 83, 80, 89, 74, 78, 66, 85, 70, 77, 90, 3, 24, 18, 3, 78, 3, 73, 74, 72, 73, 3, 2], [1, 3, 43, 68, 70, 3, 71, 80, 83, 78, 66, 85, 74, 80, 79, 84, 3, 74, 79, 3, 85, 73, 70, 3, 54, 74, 85, 77, 74, 84, 3, 72, 77, 66, 68, 74, 70, 83, 3, 68, 66, 87, 70, 3, 2], [1, 3, 35, 3, 72, 77, 66, 68, 74, 70, 83, 3, 68, 66, 87, 70, 3, 74, 84, 3, 66, 3, 68, 66, 87, 70, 3, 71, 80, 83, 78, 70, 69, 3, 88, 74, 85, 73, 74, 79, 3, 85, 73, 70, 3, 74, 68, 70, 3, 80, 71, 3, 66, 3, 72, 77, 66, 68, 74, 70, 83, 3, 16, 3, 2], [1, 3, 41, 77, 66, 68, 74, 70, 83, 3, 68, 66, 87, 70, 84, 3, 66, 83, 70, 3, 80, 71, 85, 70, 79, 3, 68, 66, 77, 77, 70, 69, 3, 74, 68, 70, 3, 68, 66, 87, 70, 84, 3, 14, 3, 67, 86, 85, 3, 85, 73, 74, 84, 3, 85, 70, 83, 78, 3, 74, 84, 3, 81, 83, 80, 81, 70, 83, 77, 90, 3, 86, 84, 70, 69, 3, 85, 80, 3, 69, 70, 84, 68, 83, 74, 67, 70, 3, 67, 70, 69, 83, 80, 68, 76, 3, 68, 66, 87, 70, 84, 3, 85, 73, 66, 85, 3, 68, 80, 79, 85, 66, 74, 79, 3, 90, 70, 66, 83, 15, 83, 80, 86, 79, 69, 3, 74, 68, 70, 3, 16, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_input = []\n",
    "for line in lines.answer:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "      temp_X.append(answer_to_index[w])\n",
    "    decoder_input.append(temp_X)\n",
    "print(decoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 35, 3, 81, 66, 83, 85, 77, 90, 3, 84, 86, 67, 78, 70, 83, 72, 70, 69, 3, 72, 77, 66, 68, 74, 70, 83, 3, 68, 66, 87, 70, 3, 80, 79, 3, 50, 70, 83, 74, 85, 80, 3, 47, 80, 83, 70, 79, 80, 3, 41, 77, 66, 68, 74, 70, 83, 3, 16, 3, 2], [3, 54, 73, 70, 3, 74, 68, 70, 3, 71, 66, 68, 66, 69, 70, 3, 74, 84, 3, 66, 81, 81, 83, 80, 89, 74, 78, 66, 85, 70, 77, 90, 3, 24, 18, 3, 78, 3, 73, 74, 72, 73, 3, 2], [3, 43, 68, 70, 3, 71, 80, 83, 78, 66, 85, 74, 80, 79, 84, 3, 74, 79, 3, 85, 73, 70, 3, 54, 74, 85, 77, 74, 84, 3, 72, 77, 66, 68, 74, 70, 83, 3, 68, 66, 87, 70, 3, 2], [3, 35, 3, 72, 77, 66, 68, 74, 70, 83, 3, 68, 66, 87, 70, 3, 74, 84, 3, 66, 3, 68, 66, 87, 70, 3, 71, 80, 83, 78, 70, 69, 3, 88, 74, 85, 73, 74, 79, 3, 85, 73, 70, 3, 74, 68, 70, 3, 80, 71, 3, 66, 3, 72, 77, 66, 68, 74, 70, 83, 3, 16, 3, 2], [3, 41, 77, 66, 68, 74, 70, 83, 3, 68, 66, 87, 70, 84, 3, 66, 83, 70, 3, 80, 71, 85, 70, 79, 3, 68, 66, 77, 77, 70, 69, 3, 74, 68, 70, 3, 68, 66, 87, 70, 84, 3, 14, 3, 67, 86, 85, 3, 85, 73, 74, 84, 3, 85, 70, 83, 78, 3, 74, 84, 3, 81, 83, 80, 81, 70, 83, 77, 90, 3, 86, 84, 70, 69, 3, 85, 80, 3, 69, 70, 84, 68, 83, 74, 67, 70, 3, 67, 70, 69, 83, 80, 68, 76, 3, 68, 66, 87, 70, 84, 3, 85, 73, 66, 85, 3, 68, 80, 79, 85, 66, 74, 79, 3, 90, 70, 66, 83, 15, 83, 80, 86, 79, 69, 3, 74, 68, 70, 3, 16, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "#디코더의 예측값과 비교하기 위한 실제값이 필요\n",
    "decoder_target = []\n",
    "for line in lines.answer:\n",
    "    t=0\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "      if t>0:\n",
    "        temp_X.append(answer_to_index[w])\n",
    "      t=t+1\n",
    "    decoder_target.append(temp_X)\n",
    "print(decoder_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "1157\n"
     ]
    }
   ],
   "source": [
    "max_question_len = max([len(line) for line in lines.question])\n",
    "max_answer_len = max([len(line) for line in lines.answer])\n",
    "print(max_question_len)\n",
    "print(max_answer_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 위해서 가장 긴 문장의 길이로 패딩하여 데이터의 길이를 동일하게 맞춤\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "encoder_input = pad_sequences(encoder_input, maxlen=max_question_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_answer_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_answer_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#원핫인코딩을 적용\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\82107\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "#훈련을 시킨다\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "encoder_inputs = Input(shape=(None, question_vocab_size))\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# encoder_outputs도 같이 리턴받기는 했지만 여기서는 필요없으므로 이 값은 버림.\n",
    "encoder_states = [state_h, state_c]\n",
    "# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 바로 은닉 상태와 셀 상태."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, answer_vocab_size))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "# 디코더의 첫 상태를 인코더의 은닉 상태, 셀 상태로 합니다.\n",
    "decoder_softmax_layer = Dense(answer_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\82107\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      " 5184/16000 [========>.....................] - ETA: 53:04 - loss: 0.7246"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 83.3 MiB for an array with shape (64, 1157, 295) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-f500707ceb78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoder_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    373\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mins\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m               \u001b[1;31m# Do not slice the training phase flag.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m               \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m               \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    527\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m     return [\n\u001b[0;32m    531\u001b[0m         \u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    527\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m     return [\n\u001b[0;32m    531\u001b[0m         \u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 83.3 MiB for an array with shape (64, 1157, 295) and data type float32"
     ]
    }
   ],
   "source": [
    "model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seq 기계 번역기 동작시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태를 이전 상태로 사용\n",
    "decoder_states = [state_h, state_c]\n",
    "# 이번에는 훈련 과정에서와 달리 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_src = dict(\n",
    "    (i, char) for char, i in question_to_index.items())\n",
    "index_to_tar = dict(\n",
    "    (i, char) for char, i in answer_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, question_vocab_size))\n",
    "    target_seq[0, 0, question_to_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition: #stop_condition이 True가 될 때까지 루프 반복\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_answer[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <sos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_answer_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트 합니다.\n",
    "        target_seq = np.zeros((1, 1, answer_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'\\t'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-18b5e1687106>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1001\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# 입력 문장의 인덱스\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0minput_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdecoded_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m35\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;34m\"-\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Question:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-70-10b545249556>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# <SOS>에 해당하는 원-핫 벡터 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtarget_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion_vocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtarget_seq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion_to_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mstop_condition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '\\t'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('Question:', lines.src[seq_index])\n",
    "    print('Answer:', lines.tar[seq_index][1:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('Answer by AI:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
